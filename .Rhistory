data = features,
y = response,
predict.fun = predict.fun,
class = "regression"
)
# Premièrement, on crée un data frame qui contient les variables explicatives
features.pois <- as.data.frame(dplyr::select(train, -amount))
# Ensuite, on crée un vecteur qui contient la variable réponse
response.pois <- as.vector(train$nclaims)
# Fonction de prédiction
predict.fun.pois <- function(model, newdata){
input_x <- model.matrix(nclaims ~ coverage +  ageph_nonscale + sex +
bm + power + agec + fuel + use +
fleet + long + lat_nonscale, newdata)
newData_x <- xgb.DMatrix(input_x, label = newdata$nclaims)
setinfo(newData_x, "base_margin", log(newdata$expo))
predict(model, newData_x)
}
# Finalement, nous créons un objet Predictor$new compatible avec iml que nous allons stocker dans un objet
# predictor.xgb.pois qui sera ensuite utilisé dans notre analyse
predictor.xgb.pois <- Predictor$new(
model = pois.xgb,
data = features.pois,
y = response.pois,
predict.fun = predict.fun.pois
)
# Premièrement, on crée un data frame qui contient les variables explicatives
features.gamma <- as.data.frame(dplyr::select(train[train$nclaims > 0,], -expo))
# Ensuite, on crée un vecteur qui contient la variable réponse
response.gamma <- as.vector(train[train$nclaims > 0,]$amount)
# Fonction de prédiction
predict.fun.gamma <- function(model, newdata){
newdatasev <- newdata[newdata$nclaims > 0,]
input_x <- model.matrix(amount ~ coverage +  ageph_nonscale + sex +
bm + power + agec + fuel + use +
fleet + long + lat_nonscale, newdatasev)
newData_x <- xgb.DMatrix(input_x, label = newdatasev$nclaims)
setinfo(newData_x, "base_margin", log(newdatasev$nclaims))
predict(model, newData_x)
}
# Finalement, nous créons un objet Predictor$new compatible avec iml que nous allons stocker dans un objet
# predictor.xgb.gamma qui sera ensuite utilisé dans notre analyse
predictor.xgb.gamma <- Predictor$new(
model = gamma.xgb,
data = features.gamma,
y = response.gamma,
predict.fun = predict.fun.gamma
)
xgb.pdp.ageph <- FeatureEffect$new(predictor.xgb.tweed, "ageph_nonscale", method="pdp", grid.size=40)
pois.pdp.ageph <- FeatureEffect$new(predictor.xgb.pois, "ageph_nonscale", method="pdp", grid.size=40)
gam.pdp.ageph <- FeatureEffect$new(predictor.xgb.gamma, "ageph_nonscale", method="pdp", grid.size=40)
## Graphique du XGboost Tweedie pour la variable la plus importante "bm"
g1 <- plot(xgb.pdp.ageph) + ggtitle("Tweedie")
## Graphique du XGboost Fréquence pour la variable la plus importante "bm"
g2 <- plot(pois.pdp.ageph) + ggtitle("Poisson")
## Graphique du XGboost Sévérité pour la variable la plus importante "bm"
g3 <- plot(gam.pdp.ageph) + ggtitle("Gamma")
gridExtra::grid.arrange(g1, g2, g3, ncol = 1, nrow = 3)
# Création aléatoire d'un échantillon
set.seed(1244,  sample.kind = "Rejection")
ind.ech <- sample(1:nrow(train), 0.1*nrow(train), FALSE)
ech.iml <- train[ind.ech,]
# Paramètres pour la Tweedie
features.ech <- as.data.frame(dplyr::select(ech.iml, -nclaims))
response.ech <- as.vector(ech.iml$amount)
# Paramètres pour la Poisson
features.pois.ech <- as.data.frame(dplyr::select(ech.iml, -amount))
response.pois.ech <- as.vector(ech.iml$nclaims)
# Paramètres pour la gamma
features.gamma.ech <- as.data.frame(dplyr::select(ech.iml[ech.iml$nclaims > 0,], -expo))
response.gamma.ech <- as.vector(ech.iml[ech.iml$nclaims > 0,]$amount)
predictor.ech.tweed <- Predictor$new(
model = tweed.xgb,
data = features.ech,
y = response.ech,
predict.fun = predict.fun,
class = "regression"
)
predictor.ech.pois<- Predictor$new(
model = pois.xgb,
data = features.pois.ech,
y = response.pois.ech,
predict.fun = predict.fun.pois,
class = "regression"
)
predictor.ech.gamma <- Predictor$new(
model = gamma.xgb,
data = features.gamma.ech,
y = response.gamma.ech,
predict.fun = predict.fun.gamma,
class = "regression"
)
xgb.ice.ageph <- FeatureEffect$new(predictor.ech.tweed, "ageph_nonscale", method="ice", grid.size=15)
xgb.ice.agec <- FeatureEffect$new(predictor.ech.pois, "ageph_nonscale", method="ice", grid.size=15)
xgb.ice.bm <- FeatureEffect$new(predictor.ech.gamma, "ageph_nonscale", method="ice", grid.size=15)
## Graphique du GBM Tweedie pour la variable la plus importante "bm"
I1 <- plot(xgb.ice.ageph) + ggtitle("Tweedie")
## Graphique du GBM Fréquence pour la variable la plus importante "bm"
I2 <- plot(xgb.ice.agec) + ggtitle("Poisson")
## Graphique du GBM Sévérité pour la variable la plus importante "bm"
I3 <- plot(xgb.ice.bm) + ggtitle("Gamma")
gridExtra::grid.arrange(I1, I2, I3, ncol = 3, nrow = 1)
set.seed(378, sample.kind = "Rejection")
int.ageph <- Interaction$new(predictor.xgb.tweed, "ageph_nonscale", grid.size = 5)
plot(int.ageph)
set.seed(47, sample.kind = "Rejection")
shapley.tweedie <- Shapley$new(predictor.xgb.tweed, x.interest = features[2,], sample.size = 10)
plot(shapley.tweedie)
# Premièrement, nous allons chercher les coefficients du GLM
coef <- summary(glm_tweed)$coefficients[,1]
# On peut ensuite venir trouver le différentiels pour l'observation
input_glm <- model.matrix(glm_tweed, train)
differentiel <- exp(input_glm[2,]*coef)
# La prime est le produit des différentiels. Validons que nous obtenons la même chose de cette façon qu'avec le produit des différentiels
prime <- prod(differentiel)
pred_obs_glm <- predict(glm_tweed, train[2,], type = "response")
c("Prod. des diff." = prime, "Prédiction du glm" = pred_obs_glm)
diff_dat <- as.data.frame(differentiel)
variables <- rownames(diff_dat)
# On enlève la prime de base pour concentrer notre analyse sur les différentiels
diff_dat <- cbind(variables, diff_dat)[-1,]
## On trace le graphique des coefficients des glms
ggplot(diff_dat, aes(y = differentiel, x = variables, fill = differentiel)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(x = "Variable", y = "Différentiel", fill= "Différentiel")+
scale_fill_continuous(low = "gold",
high = "firebrick") + theme_bw()+
geom_text(aes(label= round(differentiel, 2)))
## 1. On commence par créer les primes
P_comp<- predict.fun(tweed.xgb, newdata = test) # Notre prime à comparer est la tweedie
P_freq <- predict.fun.pois(pois.xgb, test)
P_sev <- predict.fun.gamma(gamma.xgb, test)
P_ref <- P_freq*P_sev*test$expo # Notre prime de référence est la celle obtenue avec la fréquence et la sévérité
## 2. relativité
r <- P_comp/P_ref
## 3. Trier les individus
sort_id2 <- sort(r, index.return = TRUE)$ix
## 4. On crée 5 groupes qui contiennent la même exposition (environ)
expo_cumul2 <- cumsum(test$expo[sort_id2])
cut_points2 <- sum(test$expo)/5*0:5
groups2 <- cut(expo_cumul2, cut_points2, labels=FALSE)
## 5. On calcule le Loss Ratio par groupe
test_sort2 <- test[sort_id2, ]
test_sort2$r_group <- groups2
test_sort2$P_ref <- P_ref[sort_id2]
LossRatio_table2 <- aggregate(cbind(P_ref, amount) ~ r_group, data = test_sort2, FUN = sum)
LossRatio_table2$LR_ref <- LossRatio_table2$amount/LossRatio_table2$P_ref
## 6. On trace le graphique
ggplot(LossRatio_table2, aes(x = r_group, y = LR_ref, fill = LR_ref)) + geom_bar(stat = "identity") + theme_classic() +
scale_y_continuous(labels = scales::percent) +
labs( x="Groupes de relativité", y="Loss Ratio",
caption = "Tweedie à comparer et Freq/sev comme référence") + theme(legend.position="none")
if(!("actuarialmetrics" %in% installed.packages())){
## Install the package
devtools::install_github("Olicoside/actuarialmetrics", force = TRUE)
}
+
## Load the package
library("actuarialmetrics")
devtools::install_github("Olicoside/actuarialmetrics", force = TRUE)
if(!("actuarialmetrics" %in% installed.packages())){
## Install the package
devtools::install_github("Olicoside/actuarialmetrics", force = TRUE)
}
## Load the package
library("actuarialmetrics")
P_glm <- predict(glm_tweed, type = "response", newdata = test)
Premiums  <-  data.frame("twee_glm_prem" = P_glm,
"twee_xgb_prem" = P_comp,
"freqsev_xgb_prem" = P_ref,
"expo" = test$expo,
"amount" = test$amount)
table_to_g <- get_lr_table(Premiums,
ref_name = "freqsev_xgb_prem",
comp_name = "twee_xgb_prem",
expo_name = "expo",
loss_name = "amount",
n_cuts = 6)
actuarialmetrics::lrlift_graph(table_to_g)
if(!("actuarialmetrics" %in% installed.packages())){
## Install the package
devtools::install_github("Olicoside/actuarialmetrics", force = TRUE)
}
## Load the package
library(actuarialmetrics)
P_glm <- predict(glm_tweed, type = "response", newdata = test)
Premiums  <-  data.frame("twee_glm_prem" = P_glm,
"twee_xgb_prem" = P_comp,
"freqsev_xgb_prem" = P_ref,
"expo" = test$expo,
"amount" = test$amount)
table_to_g <- get_lr_table(Premiums,
ref_name = "freqsev_xgb_prem",
comp_name = "twee_xgb_prem",
expo_name = "expo",
loss_name = "amount",
n_cuts = 6)
actuarialmetrics::lrlift_graph(table_to_g)
## On réutilise les primes de la dernière section
P_glm <- predict(glm_tweed, type = "response", newdata = test)
Premiums  <-  list("twee_glm_prem" = P_glm/test$expo,
"twee_xgb_prem" = P_comp/test$expo,
"freqsev_xgb_prem" = P_ref/test$expo)
## On crée trois variables qui contiendront l'indice pour le tri
sort_index <- lapply(Premiums, function(x) sort.int(x, index.return = TRUE)$ix)
## 2. On crée 5 groupes qui contiennent la même exposition (environ)
expo_cumul3 <- lapply(sort_index, function(x) cumsum(test$expo[x]))
cut_points3 <- sum(test$expo)/5*0:5
groups3 <- lapply(expo_cumul3, cut, cut_points3, labels=FALSE)
## 3. On calcule le montant moyen de perte par groupes
## 3.1 On créé des dataframes qui contiennent la perte annualisée et les groupes
useful_dataframes <- lapply(1:length(Premiums), function(u) data.frame("amount_year" = test$amount[sort_index[[u]]],
"group" = groups3[[u]]))
## 3.2 Utiliser la fonction `aggregate()` pour... aggréger
infos_to_graph <- lapply(useful_dataframes, function(j) aggregate(amount_year~group, FUN = function(x) sum(x)/cut_points3[2], data = j))
## 3.3 On crée le dataframe final prêt à illustrer !
Lift_graph <- data.frame(rbind(infos_to_graph[[1]], infos_to_graph[[2]], infos_to_graph[[3]]),
"model" = c(rep("GLM Tweedie", 5), rep("XGB Tweedie", 5), rep("XGB Freq/Sev", 5)))
## 4. et 5. On illustre tout les graphiques et on compare
ggplot(Lift_graph, aes(x = group, y = amount_year, fill = model, col = model)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.3) +
scale_fill_brewer(palette = "Dark2") +
scale_color_brewer(palette = "Dark2") +
theme_bw() +
stat_smooth(method=lm, se = FALSE) +
scale_y_continuous(labels = scales::dollar) +
labs(x = "Quintile à analyser", y = "Perte moyenne", caption = "Les droites de régression servent à aider à l'obtention de la conclusion à tirer du graphique")
Comp_to_graph <- data.frame("prime_twee" = P_comp,
"prime_freqsev" = P_ref)
ggplot(Comp_to_graph, aes(x = prime_freqsev, y = prime_twee)) + geom_point(alpha = 0.25, col = "red") + theme_bw() + geom_abline(intercept = 0, slope = 1) +
scale_y_continuous(labels = scales::dollar, limits = c(0, 500)) +
scale_x_continuous(labels = scales::dollar, limits = c(0, 500)) +
labs(x = "Prime prédite pour le modèle XGboost fréquence/sévérité", y = "Prime prédite pour le modèle XGboost Tweedie")
## 1. On a déjà les primes
twee_glm_prem <- P_glm
twee_xgb_prem <- P_comp
freqsev_xgb_prem <- P_ref
## 2. On calcule nos relativités "r"
r_twee <- twee_xgb_prem/twee_glm_prem
r_freqsev <- freqsev_xgb_prem/twee_glm_prem
## 3. On trie les primes et les pertes
sorted_twee_prem <- twee_glm_prem[sort.int(r_twee,
index.return = TRUE)$ix]
sorted_freqsev_prem <- twee_glm_prem[sort.int(r_freqsev,
index.return = TRUE)$ix]
sorted_losses_twee <- test$amount[sort.int(r_twee,
index.return = TRUE)$ix]
sorted_losses_freqsev <- test$amount[sort.int(r_freqsev,
index.return = TRUE)$ix]
## 4. On crée un vecteur de somme cumulative pour chacune de ces 3 valeurs
##    (divisé par le montant total pour avoir des %)
cs_twee_prem <- cumsum(sorted_twee_prem)/sum(sorted_twee_prem)
cs_freqsev_prem <- cumsum(sorted_freqsev_prem)/sum(sorted_freqsev_prem)
cs_losses_twee <- cumsum(sorted_losses_twee)/sum(sorted_losses_twee)
cs_losses_freqsev <- cumsum(sorted_losses_freqsev)/sum(sorted_losses_freqsev)
## 5. On crée un data.frame (On va séparer par "model")
gini_to_graph <- data.frame("prem" = c(cs_twee_prem, cs_freqsev_prem),
"loss" = c(cs_losses_twee, cs_losses_freqsev),
"model" = c(rep("Twee", length(cs_twee_prem)),
rep("Freq/Sev", length(cs_freqsev_prem))))
## 6. On trace le graphique
ggplot(gini_to_graph, aes(x = loss, y = prem, col = model)) + geom_line() + theme_classic() +
scale_y_continuous(labels = scales::percent, limits = c(0,1)) +
scale_x_continuous(labels = scales::percent, limits = c(0,1)) +
labs( x="% des pertes rencontrées", y="% des primes rencontrées",
caption = "comparaison des courbes de gini pour nos différents modèles") +
scale_color_brewer(palette="Dark2") +
geom_abline(intercept = 0, slope = 1, col = "grey")
## Moyen alternatif d'ajouter des variables
test_gini <- data.frame(test, twee_glm_prem, twee_xgb_prem, freqsev_xgb_prem)
## On peux maintenant créer un dataset léger qui contient ce qu'on veut
test_gini <- test_gini[, names(test_gini) %in% c("amount","twee_glm_prem", "twee_xgb_prem", "freqsev_xgb_prem")]
cplm::gini(loss = 'amount',
score = paste0(c('twee_glm', 'twee_xgb', 'freqsev_xgb'), '_prem'),
data = test_gini)
unlist(lapply(Premiums, function(k) deviance_tweedie(test$amount, k, w = test$expo, tweedie_p = tweed.p)))
library(sf)
library(ggmap)
readShapefile = function(){
belgium_shape <- sf::read_sf(dsn = path.expand('Shape file Belgie postcodes'), layer = 'npc96_region_Project1', verbose = FALSE)
belgium_shape <- sf::st_transform(belgium_shape, CRS('+proj=longlat +datum=WGS84'))
belgium_shape$id <- row.names(belgium_shape)
return(belgium_shape)
}
belgium_shape <- readShapefile()
path.expand('Shape file Belgie postcodes')
getwd()
setwd("C:/Users/olico/ulaval/13 - H23/___CHRG___ACT4114/Labos-ACT3114/9_Comparaison Modele")
sf::read_sf(dsn = path.expand('Shape file Belgie postcodes'),
layer = 'npc96_region_Project1',
verbose = FALSE)
belgium_shape <- sf::read_sf(dsn = path.expand('Shape file Belgie postcodes'),
layer = 'npc96_region_Project1')
sf::read_sf(dsn = path.expand('Shape file Belgie postcodes'),
layer = 'npc96_region_Project1')
belgium_shape <- sf::st_transform(belgium_shape, CRS('+proj=longlat +datum=WGS84'))
belgium_shape$id <- row.names(belgium_shape)
belgium_shape <- readShapefile()
readShapefile = function(){
belgium_shape <- sf::read_sf(dsn = path.expand('Shape file Belgie postcodes'),
layer = 'npc96_region_Project1')
belgium_shape <- sf::st_transform(belgium_shape, CRS('+proj=longlat +datum=WGS84'))
belgium_shape$id <- row.names(belgium_shape)
return(belgium_shape)
}
belgium_shape <- readShapefile()
comb_geo <- data.frame('long' = unname(sf::st_coordinates(belgium_shape)[,1]),
'lat' = unname(sf::st_coordinates(belgium_shape)[,2]))
belgium_shape <- readShapefile()
comb_geo <- data.frame('long' = unname(sf::st_coordinates(belgium_shape)[,1]),
'lat' = unname(sf::st_coordinates(belgium_shape)[,2]))
## On conserve en mémoire les coefficients du GLM
table_glm <- summary(glm_tweed)$coefficients
train_copy <- train
to_graph_glm <- c("(Intercept)", "long", "lat")
position <- sapply(to_graph_glm, function(k) which(row.names(table_glm) == k))
mean_linear_predictor <- mean(predict(glm_tweed)) - sum(c(mean(data$long), mean(data$lat_nonscale)) * table_glm[position[2:3], 1])
geoeff_glm <- function(u)
{
exp(mean_linear_predictor + sum(unlist(comb_geo[u, ] * table_glm[position[2:3], 1])))
}
effect_glm <- sapply(1:nrow(comb_geo), geoeff_glm)
belgium_shape@data <- cbind(belgium_shape@data, effect_glm)
## On conserve en mémoire les coefficients du GLM
table_glm <- summary(glm_tweed)$coefficients
train_copy <- train
to_graph_glm <- c("(Intercept)", "long", "lat")
position <- sapply(to_graph_glm, function(k) which(row.names(table_glm) == k))
mean_linear_predictor <- mean(predict(glm_tweed)) - sum(c(mean(data$long), mean(data$lat_nonscale)) * table_glm[position[2:3], 1])
geoeff_glm <- function(u)
{
exp(mean_linear_predictor + sum(unlist(comb_geo[u, ] * table_glm[position[2:3], 1])))
}
effect_glm <- sapply(1:nrow(comb_geo), geoeff_glm)
belgium_shape2 <- cbind(belgium_shape, effect_glm)
belgium_shape
fortify(belgium_shape)
effect_glm
belgium_shape %>%  view
belgium_shape$geometry
View(comb_geo)
belgium_shape2 <- cbind(comb_geo, effect_glm)
belgium_shape_f <- fortify(belgium_shape)
View(belgium_shape_f)
ggplot(belgium_shape2, aes(x = long, y = lat)) +
geom_polygon(aes(fill = effect_glm)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le GLM")
ggplot(belgium_shape2, aes(x = long, y = lat)) +
geom_sf(aes(fill = effect_glm)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le GLM")
ggplot(belgium_shape, aes(x = long, y = lat)) +
geom_sf(aes(fill = effect_glm)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le GLM")
readShapefile = function(){
to_return <- list('id' = row.names(belgium_shape),
'trans' = sf::st_transform(belgium_shape,
CRS('+proj=longlat +datum=WGS84')),
'raw' = sf::read_sf(dsn = path.expand('Shape file Belgie postcodes'),
layer = 'npc96_region_Project1'))
return()
}
belgium_shape <- readShapefile()
comb_geo <- data.frame('long' = unname(sf::st_coordinates(belgium_shape)[,1]),
'lat' = unname(sf::st_coordinates(belgium_shape)[,2]))
belgium_shape <- readShapefile()
readShapefile = function(){
belgium_shape <- sf::read_sf(dsn = path.expand('Shape file Belgie postcodes'),
layer = 'npc96_region_Project1')
to_return <- list('id' = row.names(belgium_shape),
'trans' = sf::st_transform(belgium_shape,
CRS('+proj=longlat +datum=WGS84')),
'raw' = belgium_shape)
return()
}
belgium_shape <- readShapefile()
comb_geo <- data.frame('long' = unname(sf::st_coordinates(belgium_shape)[,1]),
'lat' = unname(sf::st_coordinates(belgium_shape)[,2]))
comb_geo <- data.frame('long' = unname(sf::st_coordinates(belgium_shape$trans)[,1]),
'lat' = unname(sf::st_coordinates(belgium_shape$trans)[,2]))
sf::st_coordinates(belgium_shape$trans)[,1]
belgium_shape$trans
readShapefile = function(){
belgium_shape <- sf::read_sf(dsn = path.expand('Shape file Belgie postcodes'),
layer = 'npc96_region_Project1')
to_return <- list('id' = row.names(belgium_shape),
'trans' = sf::st_transform(belgium_shape,
CRS('+proj=longlat +datum=WGS84')),
'raw' = belgium_shape)
return(to_return)
}
belgium_shape <- readShapefile()
comb_geo <- data.frame('long' = unname(sf::st_coordinates(belgium_shape$trans)[,1]),
'lat' = unname(sf::st_coordinates(belgium_shape$trans)[,2]))
## On conserve en mémoire les coefficients du GLM
table_glm <- summary(glm_tweed)$coefficients
train_copy <- train
to_graph_glm <- c("(Intercept)", "long", "lat")
mean_linear_predictor <- mean(predict(glm_tweed)) - sum(c(mean(data$long), mean(data$lat_nonscale)) * table_glm[position[2:3], 1])
geoeff_glm <- function(u)
{
exp(mean_linear_predictor + sum(unlist(comb_geo[u, ] * table_glm[position[2:3], 1])))
}
effect_glm <- sapply(1:nrow(comb_geo), geoeff_glm)
belgium_shape$trans %>% view()
belgium_shape$raw %>% view()
belgium_shape$trans %>% view()
sf::st_coordinates(belgium_shape$trans) %>%  view()
map <- read_sf("https://raw.githubusercontent.com/R-CoderDotCom/data/main/shapefile_spain/spain.geojson")
View(map)
sf::st_coordinates(belgium_shape$trans) %>%  view()
sf::st_coordinates(belgium_shape$trans) %>%  nrow()
sf::st_coordinates(belgium_shape$trans) %>%  view()
sf::st_coordinates(belgium_shape$trans) %>% lapply(function(x){length(unique(x))})
sf::st_coordinates(belgium_shape$trans)$L1 %>% unique %>% length
sf::st_coordinates(belgium_shape$trans) %>% unique %>% length
sf::st_coordinates(belgium_shape$trans) %>%  ncol
sf::st_coordinates(belgium_shape$trans) %>%  colnames
sf::st_coordinates(belgium_shape$trans)[[L1]] %>% unique %>% length
sf::st_coordinates(belgium_shape$trans)[['L1']] %>% unique %>% length
a <- sf::st_coordinates(belgium_shape$trans)
sf::st_coordinates(belgium_shape$trans)[, 3] %>% unique %>% length
sf::st_coordinates(belgium_shape$trans)[, 4] %>% unique %>% length
sf::st_coordinates(belgium_shape$trans)[, 4] %>% unique %>% length
sf::st_coordinates(belgium_shape$trans)[, 5] %>% unique %>% length
sf::st_point(belgium_shape$trans) %>% nrow
sf::st_point_on_surface(belgium_shape$trans) %>% nrow
sf::st_point_on_surface(belgium_shape$trans) %>% view
sf::st_coordinates(belgium_shape$trans) %>%  as.data.frame %>% group_by(L3) %>% summarise(long = mean(long), lat = mean(lat)) %>%  nrow()
sf::st_coordinates(belgium_shape$trans) %>%  as.data.frame %>% group_by(L3) %>% summarise(long = mean(long), lat = mean(lat))
sf::st_coordinates(belgium_shape$trans) %>%  data.frame %>% group_by(L3) %>% summarise(long = mean(long), lat = mean(lat))
sf::st_coordinates(belgium_shape$trans) %>% colnames
sf::st_coordinates(belgium_shape$trans) %>%  data.frame %>% group_by(L3) %>% summarise(long = mean(X), lat = mean(Y))
sf::st_coordinates(belgium_shape$trans) %>%  data.frame %>% group_by(L3) %>% summarise(long = mean(X), lat = mean(Y)) %>% ungroup()
sf::st_coordinates(belgium_shape$trans) %>%  data.frame %>% group_by(factor(L3)) %>% summarise(long = mean(X), lat = mean(Y)) %>% ungroup()
sf::st_coordinates(belgium_shape$trans) %>%  data.frame %>% pull(L3) %>%  table
sf::st_coordinates(belgium_shape$trans) %>%  aggregate(L3 ~X + Y, FUN = mean)
sf::st_coordinates(belgium_shape$trans) %>%  aggregate(c(X, Y) ~ L3, FUN = mean)
sf::st_coordinates(belgium_shape$trans) %>%  aggregate(c(X, Y) ~ L3, FUN = mean, data = .)
sf::st_coordinates(belgium_shape$trans) %>%  aggregate(X+ Y ~ L3, FUN = mean, data = .)
sf::st_coordinates(belgium_shape$trans) %>%  aggregate(cbind(X,Y) ~ L3, FUN = mean, data = .)
(sf::st_coordinates(belgium_shape$trans) %>%  aggregate(cbind(X,Y) ~ L3, FUN = mean, data = .))$X
belgium_shape <- readShapefile()
coord <- sf::st_coordinates(belgium_shape$trans) %>%  aggregate(cbind(X,Y) ~ L3, FUN = mean, data = .)
comb_geo <- data.frame('long' = unname(coord$X),
'lat' = unname(coord$Y)))
comb_geo <- data.frame('long' = unname(coord$X),
'lat' = unname(coord$Y))
to_graph_glm <- c("(Intercept)", "long", "lat")
position <- sapply(to_graph_glm, function(k) which(row.names(table_glm) == k))
mean_linear_predictor <- mean(predict(glm_tweed)) - sum(c(mean(data$long), mean(data$lat_nonscale)) * table_glm[position[2:3], 1])
geoeff_glm <- function(u)
{
exp(mean_linear_predictor + sum(unlist(comb_geo[u, ] * table_glm[position[2:3], 1])))
}
effect_glm <- sapply(1:nrow(comb_geo), geoeff_glm)
belgium_shape2 <- cbind(belgium_shape$trans, effect_glm)
belgium_shape_f <- fortify(belgium_shape2)
View(belgium_shape_f)
View(belgium_shape2)
ggplot(belgium_shape2, aes(x = long, y = lat)) +
geom_sf(aes(fill = effect_glm)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le GLM")
ggplot(belgium_shape2) +
geom_sf(aes(fill = effect_glm)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le GLM")
geoeff_xgb <- function(u)
{
train_copy$long <- unlist(rep(comb_geo[u, 1], nrow(train_copy)))
train_copy$lat_nonscale <- unlist(rep(comb_geo[u, 2], nrow(train_copy)))
mean(predict.fun(tweed.xgb, newdata = train_copy))
}
effect_xgb <- sapply(1:nrow(comb_geo), geoeff_xgb)
belgium_shape2 <- cbind(belgium_shape$trans, effect_xgb)
ggplot(belgium_shape_f) +
geom_sf(aes(fill = effect_xgb)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le XGboost")
belgium_shape2 <- cbind(belgium_shape$trans, effect_xgb)
ggplot(belgium_shape2) +
geom_sf(aes(fill = effect_xgb)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le XGboost")
ggplot(belgium_shape2) +
geom_sf(col = 'transparent', aes(fill = effect_xgb)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le XGboost")
ggplot(belgium_shape2) +
geom_sf(col = 'white',aes(fill = effect_xgb)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le XGboost")
ggplot(belgium_shape2) +
geom_sf(aes(fill = effect_xgb)) +
scale_fill_gradient(low='yellow',high='red') +
theme_void() + labs(fill='Prime moyenne') +
labs(title =  "Effet des coordonnées géographiques pour le XGboost")
remove.packages("rgdal")
remove.packages("htmltools")
